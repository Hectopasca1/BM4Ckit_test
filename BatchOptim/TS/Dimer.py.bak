''' Dimer Algo. for transition state searching. J. Chem. Phys. 1999, 111: 7010.'''

# ruff: noqa: E701, E702, E703
from typing import Iterable, Dict, Any, List, Literal, Optional, Callable, Sequence, Tuple  # noqa: F401
import time
import warnings

import torch as th
from torch import nn
from torch.nn import functional as Fn
from .._utils._warnings import FaildToConvergeWarning

from ..minimize import CG, QN, FIRE

class Dimer:
    """

    """
    def __init__(self,
                 E_threshold:float=1e-3,
                 E_thres_rot:float=1e-2,
                 F_threshold:float=0.05,
                 maxiter_trans:int=100,
                 maxiter_rot:int=10,
                 steplength:float=1., 
                 dx:float=1.e-3,
                 device:str|th.device='cpu', 
                 verbose:int=2):
        
        self.E_threshold = E_threshold
        self.E_thres_rot = E_thres_rot
        self.F_threshold = F_threshold
        self.maxiter_trans = maxiter_trans
        self.maxiter_rot = maxiter_rot
        self.steplength = steplength
        self.dx = dx
        self.device = device
        self.verbose = verbose
        pass
    
    def _external_force(self, X, grad_func, X0,
                        grad_func_args:Sequence=tuple(), grad_func_kwargs=None,
                        is_grad_func_contain_y:bool=True,
                        y=None, k:float=1.e+3):
        if grad_func_kwargs is None:
            grad_func_kwargs = dict()
        if is_grad_func_contain_y:
            g = grad_func(X, y, *grad_func_args, **grad_func_kwargs)
        else:
            g = grad_func(X, *grad_func_args, **grad_func_kwargs)
        return g + 2 * k * ((th.linalg.norm(X - X0, dim=(-2, -1)) - 3.)/th.linalg.norm(X - X0, dim=(-2, -1))) * (X - X0)
    
    def _rotate(self, ):
        pass

    def _transition(self, ):
        pass

    def run(self,
            func:Any|nn.Module, X:th.Tensor, X_diff:th.Tensor, grad_func:Any|nn.Module=None,
            func_args:Sequence=tuple(), func_kwargs=None, grad_func_args:Sequence=tuple(), grad_func_kwargs=None,
            is_grad_func_contain_y:bool=True, output_grad:bool=False,
            fixed_atom_tensor:Optional[th.Tensor]=None, ):
        
        if grad_func_kwargs is None:
            grad_func_kwargs = dict()
        if func_kwargs is None:
            func_kwargs = dict()

        t_main = time.perf_counter()
        n_batch, n_atom, n_dim = X.shape
        if grad_func is None:
            is_grad_func_contain_y = True
            def grad_func_(x, y, grad_shape=None):
                if grad_shape is None: 
                    grad_shape = th.ones_like(y)
                g = th.autograd.grad(y, x, grad_shape)
                return g[0]
        else:
            grad_func_ = grad_func
        # Selective dynamics
        if fixed_atom_tensor is None:
            atom_masks = th.ones_like(X, device=self.device)
        elif fixed_atom_tensor.shape == X.shape:
            atom_masks = fixed_atom_tensor.to(self.device)
        else:
            raise RuntimeError(f'fixed_atom_tensor (shape: {fixed_atom_tensor.shape}) does not have the same shape of X (shape: {X.shape}).')
        #atom_masks = atom_masks.flatten(-2, -1).unsqueeze(-1)  # (n_batch, n_atom*n_dim, 1)
        # other check
        if (not isinstance(self.maxiter_trans, int)) or (not isinstance(self.maxiter_rot, int))\
                or (self.maxiter_trans <= 0) or (self.maxiter_rot <= 0):
            raise ValueError(f'Invalid value of maxiter: {self.maxiter_trans}. It would be an integer greater than 0.')
        
        # set variables device
        if isinstance(func, nn.Module):
            func = func.to(self.device)
            func.zero_grad()
        if isinstance(grad_func_, nn.Module):
            grad_func_ = grad_func_.to(self.device) 
        X = X.to(self.device)
        X_diff = X_diff.to(self.device)
        for _args in (func_args, func_kwargs.values(), grad_func_args, grad_func_kwargs.values()):
            for _arg in _args:
                if isinstance(_arg, th.Tensor):
                    _arg.to(self.device)

        plist = list()  # TEST <<<<
        minimizer = FIRE(self.E_thres_rot, 1.e+5, self.maxiter_rot, 0.2, device=self.device, verbose=0)
        for i in range(self.maxiter_trans):
            t_st = time.perf_counter()
            # ORIGINAL X <<<<
            X1 = X  # (n_batch, n_atom*n_dim)
            X2 = X + X_diff
            #NI = - nn.functional.normalize(X_diff, dim=-1)  # (n_batch, n_atom*n_dim), the element vectors.
            #dX = 0.5 * (X_diff.unsqueeze(1) @ NI.unsqueeze(-1))  # (n_batch, 1, n_atom*n_dim)@(n_batch, n_atom*n_dim, 1)
            #dX.squeeze_(-1)  # (n_batch, 1)

            # Rotate to minimum
            _wrap_force_args = [grad_func_, X1]  # wrap forces to external forces
            _wrap_force_args.extend(grad_func_args)
            print('-' * 100)
            energies, X2, F2 = minimizer.run(func, X2, self._external_force, func_args, func_kwargs,
                                         _wrap_force_args, grad_func_kwargs, is_grad_func_contain_y, 
                                         True, atom_masks)
            print('-' * 100 + '\n')

            # Transition to maximum
            y1 = func(X1, *func_args, **func_kwargs)
            if is_grad_func_contain_y:
                F1 = - grad_func_(X1, y1, *grad_func_args, **grad_func_kwargs) * atom_masks
            else:
                F1 = - grad_func_(X1, *grad_func_args, **grad_func_kwargs) * atom_masks

            X1_ = X1.flatten(-2, -1)  # '_' means the flatten variables
            F1_ = F1.flatten(-2, -1)
            X2_ = X2.flatten(-2, -1)
            F2_ = F2.flatten(-2, -1)
            FR_ = (F1_ + F2_)/2  # Total force of center
            X_diff = X2 - X1
            NI = Fn.normalize(X2_ - X1_, dim=-1)  # element direction vectors, (n_batch, n_atom * n_dim)
            rx1x2 = ((X2_ - X1_).unsqueeze(1) @ NI.unsqueeze(-1))/2.  # (n_batch, 1, 1)
            Curv = ((F2_ - F1_).unsqueeze(1) @ NI.unsqueeze(-1))/(2 * rx1x2)  # (n_batch, 1, 1)
            Torque = F2_ - (F2_.unsqueeze(1) @ NI.unsqueeze(-1)) * NI

            plist.append(X1[:, None, :, 0].numpy(force=True))

            # Judge Criterion
            print(f'Iteration: {i+1}\n'
                  f'Curvature: {Curv.squeeze().numpy(force=True)}\nMax Torque: {th.max(th.abs(Torque)).numpy(force=True)}\n'
                  f'Max Force: {th.max(FR_)}\nX_diff: {th.linalg.norm(X_diff)}')
            if th.max(th.abs(FR_)) <= self.F_threshold:
                print('Converged')
                break
            # 1D Newton Search
            dX_ = th.where(Curv > 0.,
                          - (FR_.unsqueeze(1) @ NI.unsqueeze(-1)) * NI,
                           (FR_ - 2 * (FR_.unsqueeze(1) @ NI.unsqueeze(-1))) * NI)  # (n_batch, n_atom * n_dim)
            steplength = 0.03

            with th.no_grad():
                X = X + (steplength * dX_).reshape(n_batch, n_atom, n_dim)
            X.detach_()
            X.requires_grad_()

        return energies, X2, plist  # TEST <<<<<<
            
        pass